\documentclass[oneside,14pt]{extarticle}

% Config
\input{config}

\begin{document}

% Title page
\input{title}
\setcounter{page}{2}

% Toc
\tableofcontents
\clearpage

% Contents
\section{Вступ}

В даній роботі описана задача, що виникає в економіці\cite{David:1998} при моделюванні поведінки систем підприємств, що взаємодіють між собою. В кожен момент часу підприємство може обирати технологію виробництва, яка впливає на його власний стан, а також на поведінку інших підприємств у майбутньому.

Метою роботи є побудова і програмна реалізація ефективної моделі, яка відповідатиме на питання, яку технологію слід обирати кожному з підприємств у кожний момент часу, щоб мінімізувати свої витрати (або максимізувати дохід).







\section{Постановка задачі}

\label{sec:description}

У загальному випадку система описується\cite{Chornei:2005} для дискретного часу \(t = 0, 1, \ldots \) скінченною множиною економічних агентів \(V\) і неорієнтованим графом їхніх взаємодій \(G = (V, E)\), скінченною множиною \(X^t = \times_{v \in V}X_v^t,\ X_v^t = \{x^{t,1}_v, x^{t,2}_v, \ldots, x^{t,n_v}_v\}\) можливих станів агента \(v \in V\) у момент часу \(t\), множиною рішень (про вибір відповідних технологій) \(\Delta_v^t: \times_{t=0,1,\ldots}X^t \rightarrow U_v^t\), де \(U_v^t\) – скінченна множина можливих дій, та відповідних їм функцій витрат (доходів) \(r_v^t: X_v^t \times U_v^t \rightarrow \{u\ |\ u \in \mathbb{R}, |u| < C\}\) для агента \(v \in V\) у момент часу~\(t\).








\subsection{Припущення щодо системи}

Для побудови ефективної моделі, щодо системи буде додатково введено ряд обмежень.

\begin{description}

    \item[Стохастичність.] Для всіх \(v \in V\)
    \[ \xi_v = \{\xi^t_v\ |\ t = 0, 1, \ldots \}\] 
    \[ (\Omega, \mathcal{F}, P).\quad \xi_v^t : X_v \rightarrow [0, 1]\ ,\]
    де \( \xi_v \) – стохастичний процес, що визначає стан \(v\) у кожен момент часу \(t\). 
    
    \item[Локальність.] Для всіх \(v \in V\) 
    \[\Delta_v^{t+1} = \Delta_v^{t+1}(x^0, x^1, \ldots, x^t) = \Delta_v^{t+1}(x_{\tilde{N}(v)}^0, x_{\tilde{N}(v)}^1, \ldots, x_{\tilde{N}(v)}^t)\]
    \begin{align*}
    P(\xi_v^{t+1} = x_v\ &|\ \xi^0 = x^0, \Delta^0 = u^0,\ \ldots, \xi^t = x^t, \Delta^t = u^t) = \\
	= P(\xi_v^{t+1} = x_v\ &|\ \xi_{\tilde N(v)}^0 = x_{\tilde N(v)}^0, \Delta_{\tilde N(v)}^0 = u_{\tilde N(v)}^0,\ \ldots,\\ &\xi_{\tilde N(v)}^t = x_{\tilde N(v)}^t, \Delta_{\tilde N(v)}^t = u_{\tilde N(v)}^t)
	\end{align*}
	Локальність є природньою в контексті задачі, оскільки за визначенням системи, усі агенти, що не взаємодіють з даним, не чинять на нього безпосереднього впливу. Звідси, агентові для прийняття рішення достатньо знати стани тільки тих агентів, з якими він взаємодіє.
	
    \item[Синхронність.] Для всіх \(W \subset V\)
    \begin{align*}
    &P(\xi^{t+1}_W = x_W\ |\ \xi^t = x^t, \Delta^t = u^t) = \\
    = &\prod_{w \in W} P(\xi^{t+1}_w = x_w\ |\ \xi^t = x^t, \Delta^t = u^t)
    \end{align*}
    Усі агенти системи переходять у свій наступний стан одночасно. Це припущення узгоджується з інтерпретацією \(t\) як конкретного моменту в часі, однакового для всіх агентів.

	\item[Повнота стану.] (припущення Маркова) Для всіх \(v \in V\)
	\[\Delta_v^{t+1}(x_{\tilde{N}(v)}^0, x_{\tilde{N}(v)}^1, \ldots, x_{\tilde{N}(v)}^t) = \Delta_v^{t+1}(x_{\tilde{N}(v)}^t)\]
	\begin{align*}
	P(\xi_v^{t+1} = x_v\ &|\ \xi_{\tilde N(v)}^0 = x_{\tilde N(v)}^0, \Delta_{\tilde N(v)}^0 = u_{\tilde N(v)}^0,\ \ldots,\\ &\xi_{\tilde N(v)}^t = x_{\tilde N(v)}^t, \Delta_{\tilde N(v)}^t = u_{\tilde N(v)}^t) = \\
	= P(\xi_v^{t+1} = x_v\ & |\ \xi_{\tilde N(v)}^t = x_{\tilde N(v)}^t, \Delta_{\tilde N(v)}^t = u_{\tilde N(v)}^t)
	\end{align*}
	Припущення є природнім, оскільки для прийняття рішення про вибір технології найважливішим є стан сусідніх агентів у попередній момент часу, тоді як повною історією їх рішень можна знехтувати.
	
    \item[Інваріантність рішень, та просторів станів і можливих дій] Для всіх \(v \in V\), у кожен момент часу \(t', t'' = 0, 1, ...\ \)
    \begin{align*}
      X_v^{t'} &= X_v^{t''} = X_v \\
      U_v^{t'} &= U_v^{t''} = U_v \\
	  \Delta_v^{t'} &= \Delta_v^{t''} = \Delta_v
    \end{align*}
	Оскільки в реальних системах набір доступних технологій змінюється рідко (внаслідок інновацій, або застаріння технологій), змінністю простору можливих дій, так само, як і змінністю простору станів, можна знехтувати в багатьох випадках.
	
	Вважитемо також, що для кожного агента \(v \in V\) всі можливі дії \(U_v\) допустимі.

\end{description}







\subsection{Марківська модель}

Для неорієнтованого графа \(G = (V, E)\) позначимо як \(N(v)\) множину вершин, що з’єднані з вершиною \(v \in V\), і як \(\tilde{N}(v)\) — множину \(N(v)\) разом із самою \(v\):
\[N(v) = \{w\ |\ \{v, w\} \in E\},\ \tilde{N}(v) = N(v) \cup \{v\}\]

\begin{definition}
Неорієнтований граф \(G = (V, E)\), множина випадкових величин, визначених на імовірнісному просторі \((\Omega, \mathcal{F}, P),\ \xi_v : X_v \rightarrow [0, 1]\ ,\ v \in V\) утворюють \textit{марківське випадкове поле}, якщо для всіх \(v \in V\):
\[P(\xi_v = x_v\ |\ \xi_{V\setminus\{v\}} = x_{V\setminus\{v\}}) = P(\xi_k = x_v\ |\ \xi_{N(v)} = x_{N(v)})\]
\end{definition}

Поняття можна розширити до стохастичних процесів.
\begin{definition}
\label{def:mrftime}
Нехай \(\xi = \{\xi^t\ |\ t=0,1,\ldots\},\ v \in V \) — марківський процес з дискретним часом. 
Якщо виконуються:

\begin{enumerate}
    \item (Локальність) Для всіх \(v \in V\)
    \begin{align*}
    &P(\xi_v^{t+1} = x_v\ |\ \xi^0 = x^0, \xi^1 = x^1, \ldots, \xi^t = x^t) = \\
    = &P(\xi_v^{t+1} = x_v\ |\ \xi_{\tilde N(v)}^t = x_{\tilde N(v)}^t )
    \end{align*}
    \item (Синхронність) Для всіх \(W \subset V\) \[P(\xi^{t+1}_W = x_W\ |\ \xi^t = x^t) = \prod_{w \in W} P(\xi^{t+1}_w = x_w\ |\ \xi^t = x^t) \]
\end{enumerate}

Тоді процес \(\xi\) разом із графом \(G = (V, E)\) утворює \textit{марківське випадкове поле із синхронними компонентами, що локально взаємодіють}, або \textit{марківське випадкове поле з дискретним часом}. 
\end{definition}

Відразу з означення маємо, що для будь-якого \(W \subset V\)
\[P(\xi_W^{t+1} = x_W^{t+1}\ |\ \xi^t = x^t) = \prod_{w \in W} P(\xi_w^{t+1} = x_w^{t+1}\ |\ \xi_{\tilde{N}(w)}^t = x_{\tilde{N}(w)}^t)\]

Нехай \(V\) – скінченна множина агентів, що приймають рішення. \(U_v\) — скінченний простір можливих дій для агента \(v \in V\), причому \(U_v\) незалежний від часу, \(\Delta_v^t: \times_{t=0,1,\ldots}X^t \rightarrow U_v\) – рішення агента \(v\), залежні від історії станів. 

\begin{definition}
Стратегія \(\delta = \{\delta_v\ |\ v \in V\}\), де \(\delta_v = \{\Delta_v^t\ |\ t=0,1,\ldots\}\), називається локальною, якщо для всіх \(v \in V,\ t = 0, 1, \ldots\)
\[\Delta_v^t = \Delta_v^t(x_{\tilde{N}(v)}^0, \ldots, x_{\tilde{N}(v)}^t)\]
\end{definition}

\begin{definition}
Локальна стратегія називається марківською, якщо для всіх \(v \in V,\ t = 0, 1, \ldots\) 
\[\Delta_v^{t+1} = \Delta_v^{t+1}(x_{\tilde{N}(v)}^t)\]
\end{definition}

\begin{definition}
Cтратегія називається стаціонарною, якщо для всіх \(v~\in~V,\ t', t'' = 0, 1, \ldots,\)
\[\Delta_v^{t'} = \Delta_v^{t''}\]
\end{definition}

Якщо застосувати локальну марківську стратегію \(\delta\) до марківського випадкового поля \(\xi\) відносно графа \(G\), то пара \((\xi, \delta)\) утворює керований марківський ланцюг. Якщо при цьому стратегія \(\delta\) – стаціонарна, то ланцюг однорідний.

	Аналогічно до означення \ref{def:mrftime}, визначимо контрольоване марківське випадкове поле у часі.
	
\begin{definition}
\label{def:ctrlmrftime}
Нехай (\(\xi, \delta \)) — керований стохастичний процес із графом взаємодії \(G = (V, E)\), де \(\delta\) – локальна марківська стратегія. Якщо виконується:
\begin{enumerate}
    \item (Локальність) Для всіх \(v \in V\)
	\begin{align*}
    P(\xi_v^{t+1} = x_v\ &|\ \xi^0 = x^0, \Delta^0 = u^0,\ \ldots, \xi^t = x^t, \Delta^t = u^t) = \\
	= P(\xi_v^{t+1} = x_v\ &|\ \xi_{\tilde N(v)}^t = x_{\tilde N(v)}^t, \Delta_{\tilde N(v)}^t = u_{\tilde N(v)}^t)
	\end{align*}
	\item (Синхронність) Для всіх \(W \subset V\)
	\[P(\xi^{t+1}_W = x_W\ |\ \xi^t = x^t, \Delta^t = u) = \prod_{w \in W} P(\xi^{t+1}_w = x_w\ |\ \xi^t = x^t, \Delta^t = u) \]
\end{enumerate}
Тоді \((\xi, \delta)\) разом із графом \(G=(V,E)\) утворює \textit{кероване марківське поле з дискретним часом}.
\end{definition}

Як наслідок з умов означення, маємо для будь-якого \(W \subset V\)
\begin{align*}
&P(\xi_W^{t+1} = x_W\ |\ \xi^t = x^t, \Delta^t = u^t) = \\
= &\prod_{w \in W} P(\xi^{t+1} = x_w\ |\ \xi_{\tilde{N}(w)}^t = y_{\tilde{N}(w)}, \Delta_w^t(\xi_{\tilde{N}(w)}^t) = u_w)
\end{align*}

\begin{definition} 
Позначимо
\begin{align*}
Q_w(x_w / y_{\tilde{N}(w)}, u_w) &= P(\xi^{t+1} = x_w\ |\ \xi_{\tilde{N}(w)}^t = y_{\tilde{N}(w)}, \Delta_w^t(\xi_{\tilde{N}(w)}^t) = u_w) \\
Q_W(x_W / y, u) &= \prod_{w \in W} Q_w(x_w / y_{\tilde{N}(w)}, u_w)
\end{align*}
Покладаючи \(W = V\) для \( Q_W(x_W / y, u) \), назвемо ядром переходу
\[Q(x/y,\ u) = P(\xi^{t+1} = x\ |\ \xi^t = y, u^t = u)\]
\[\sum_{x \in X}Q(x/y, u) = 1,\ y \in X\]
\end{definition}

Визначений вище керований стохастичний процес \((\xi, \delta)\) відносно графа взаємодії \(G = (V, E)\) утворює кероване марківське поле з дискретним часом.








\subsection{Знаходження оптимальної стратегії}

Сформулюємо задачу знаходження стратегії, що мінімізує витрати підприємств. Нехай \(E_y^\delta\) – математичне сподівання, що відповідає процесу \((\xi, \delta)\) за початкового стану \(\xi^0 = y\).  Тоді \(C_T^\delta\) — середні очікувані витрати за час \(T\):
\begin{align*}
C_T^{\delta} = &E\left[\frac{1}{T+1}\sum_{t=0}^T r(\xi^t, \Delta^t(\xi^0, \ldots, \xi^t))\right] \\
 = &E_y^\delta\ \frac{1}{T+1}\ \sum_{t=0}^T r(\xi^t, \Delta^t(\xi^0, \ldots, \xi^t))
\end{align*}

Задача полягає в знаходженні оптимальної стратегії \(\delta^*\), яка мінімізує \(C_T^{\delta}(y)\) при \(T\rightarrow\infty\) для всіх \(y \in X\):
\begin{gather*}
R_y^\delta = \lim\limits_{T \rightarrow \infty} \sup C_T^{\delta}(y) \\
\delta^* = R_y^{\delta^*},\ y \in X
\end{gather*}

Нехай \(D_y^{\delta}(u)\) – імовірність вибору дії \(u\) за попереднього стану системи \(y\) для стратегії \(\delta\):
\begin{gather*}
D_y^{\delta}(u) = P(\Delta^{t+1} = u\ |\ \xi^t = y) \\ \sum_{u \in U} D_y^\delta(u) = 1,\ y \in X
\end{gather*}

Функція \(D_y^\delta\) однозначно визначає стратегію \(\delta\).

Згідно з результатом у \cite{Chornei:2005}, для керованого марківського поля зі скінченними просторами станів та скінченними й інваріантними у часі просторами можливих дій, існує оптимальна стратегія, яка є нерандомізованою, стаціонарною, та марківською. Таким чином, при пошуку оптимальної \(\delta^*\) можемо обмежитися лише класом \textit{нерандомізованих} стратегій.

Розподіл \(D_y^{\delta^*}\) у такому випадку є виродженим:
\[D_y^{\delta^*}(u) = \begin{cases}
1,&\ u = u_y^* \\
0,&\ u \neq u_y^*
\end{cases}
\]

Оскільки кероване марківське поле за умови стаціонарності стратегії утворює однорідний марківський ланцюг, \(\delta^*\) можна знайти, використовуючи методи лінійного програмування \cite{Knopov:1998}.

Покладемо \(z_{xu}\):
\[z_{xu} = \pi_x\ D_x^{\delta^*}(u) = \left[\sum_{u \in U} z_{xu}\right]\ D_x^{\delta^*}(u)\]

Тоді задача лінійного програмування для знаходження \(z_{xu}\) формулюється так:
\[\min \sum_{x \in X} \sum_{u \in U} r(x, u)\ z_{xu}\]
З обмеженнями:
\begin{gather*}
\sum_{u \in U} z_{xu} = \sum_{y \in X} \sum_{u \in U} Q(x/y, u)\,z_{yu},\ x \in X \\
\sum_{x \in X} \sum_{u \in U} z_{xu} = 1 \\
z_{xu} \geq 0,\ x \in X, u \in U
\end{gather*}

Враховуючи нерандомізованість, 
\[D_x^{\delta^*}(u) = \begin{cases}
1,&\ z_{xu} \neq 0 \\
0,&\ z_{xu} = 0
\end{cases}\]

Аналогічно формулюватиметься задача максимізації доходів.










\clearpage
\section{Програмна реалізація}

Реалізуємо задачу знаходження оптимальної стратегії вибору технологій за допомогою бібліотеки для моделювання задач лінійного програмування \texttt{PuLP}\cite{PuLP} для мови Python.

Програма отримує на вхід кількість станів системи \(|\times_{v \in V} X_v| = |X|\), кількість можливих дій системи \(|\times_{v \in V}U_v| = |U|\),  тривимірний масив ядер переходу \(Q\) розмірності \(|X| \times |X| \times |U| \), та матрицю витрат \(R\) розмірності \(|X| \times |U|\), де \(R_{ij} = r(x_i, u_j)\). Як результат повертає множину пар \(\{(x, u)\ |\ D_x^{\delta^*}(u) = 1\}\), яка задає оптимальну стратегію \(\delta^*\).

\begin{example}
Нехай \(G = (V, E)\), кількість вершин \(|V| = 2\), множина можливих станів для кожного агента \(X_v = \{a, b\}\), множина можливих впливів \(U_v = \{1, 2\}\). Тоді \(X = \{(a, a), (a, b), (b, a), (b, b)\}\), \(|X| = 4\), \(U = \{(1, 1), (1, 2), (2, 1), (2, 2)\}\), \(|U| = 4\). Відповідно, маємо 4 матриці ядер переходу \(Q^u\) розмірності \(4 \times 4\). Позначимо
\begin{gather*}
x_0 = (a, a),\ x_1 = (a, b),\ x_2 = (b, a),\ x_3 = (b, b), \\
u_0 = (1, 1),\ u_1 = (1, 2),\ u_2 = (2, 1),\ u_3 = (2, 2)
\end{gather*}
Тоді матриці ядер переходу \(Q^u\):
\[Q^{u} = \left(\begin{matrix}
Q(x_0 / x_0, u) & Q(x_1 / x_0, u) & Q(x_2 / x_0, u) & Q(x_3 / x_0, u) \\
Q(x_0 / x_1, u) & Q(x_1 / x_1, u) & Q(x_2 / x_1, u) & Q(x_3 / x_1, u) \\
Q(x_0 / x_2, u) & Q(x_1 / x_2, u) & Q(x_2 / x_2, u) & Q(x_3 / x_2, u) \\
Q(x_0 / x_3, u) & Q(x_1 / x_3, u) & Q(x_2 / x_3, u) & Q(x_3 / x_3, u) \\
\end{matrix}\right)\]
Матриця витрат \(R\) розмірності \(4 \times 4\):
\[R = \left(\begin{matrix}
r(x_0, u_0) & r(x_0, u_1) & r(x_0, u_2) & r(x_0, u_3)\\
r(x_1, u_0) & r(x_1, u_1) & r(x_1, u_2) & r(x_1, u_3)\\
r(x_2, u_0) & r(x_2, u_1) & r(x_2, u_2) & r(x_2, u_3) \\
r(x_3, u_0) & r(x_3, u_1) & r(x_3, u_2) & r(x_3, u_3) \\
\end{matrix}\right)\]
На виході програми отримаємо 4 пари значень \((x, u)\), для яких \(\Delta_x^{\delta^*}(u) = 1\).
\end{example}

Частково наведемо основну процедуру знаходження стратегії \texttt{find\_optimal\_strategy} на мові Python.

Оголошення функції та ініціалізація об’єктів:
\begin{lstlisting}
def find_optimal_strategy(states, controls, costs, kernels):
    X = range(states)
    U = range(controls)
    R = costs
    Q = kernels
	
    # LP object
    optm = LpProblem("Optimal strategy", sense=LpMinimize)

    # Variables (continuous in range [0, 1])
    Z = [[LpVariable("({},{})".format(x, u), 0, 1) \
                for u in U] for x in X]
\end{lstlisting}

Задання оптимізованої функції та обмежень:
\begin{lstlisting}
    # Objective
    optm.objective = sum(np.dot(Z[x], R[x]) for x in X)


    # Constraints
    for x in X:
        cn = (sum(Z[x]) == sum(Q[y][x][u]*Z[y][u] \
                                  for u in U for y in X))
        optm.add(cn)
       
    cn = sum(Z[x][u] for u in U for x in X) == 1
    optm.add(cn)

    optm.solve()
	
    return [(x, u) for u in U for x in X \
                   if value(Z[x][u]) != 0]
\end{lstlisting}

Хоча алгоритм задано не у векторизованій формі, програмне забезпечення для розв’язку задач ЛП, яке використовується бібліотекою \texttt{PuLP} — \texttt{COIN-OR}, \texttt{GLPK} тощо — автоматично векторизує оптимізовану функцію та обмеження, тому ефективність при такому заданні не втрачається. За замовчуванням \texttt{PuLP} використовує \texttt{COIN-OR}\cite{COIN}, інші варіанти можна обрати при виклику \texttt{optm.solve(solver=CUSTOM\_SOLVER)}.

Повний код програми можна знайти у Git-репозиторії за посиланням \texttt{http://github.com/bogdan-kulynych/mrf-in-economics}.

\begin{example} В умові з попереднього прикладу, нехай ядра переходу задаються таким чином:
\[Q^{u_0} = \left(\begin{matrix}
0.1 & 0.1 & 0.4 & 0.4 \\
0.7 & 0.1 & 0.1 & 0.1 \\
0.3 & 0.1 & 0.2 & 0.4 \\
0.1 & 0.5 & 0.2 & 0.2 \\
\end{matrix}\right),\
Q^{u_1} = \left(\begin{matrix}
0.2 & 0.1 & 0.3 & 0.4 \\
0.1 & 0.  & 0.  & 0.9 \\
0.3 & 0.5 & 0.  & 0.2 \\
0.1 & 0.1 & 0.7 & 0.1 \\
\end{matrix}\right) \]

\[Q^{u_2} = \left(\begin{matrix}
0.7 & 0.1 & 0.1 & 0.1 \\
0.2 & 0.2 & 0.5 & 0.1 \\
0.9 & 0.  & 0.1 & 0.  \\
0.  & 0.6 & 0.2 & 0.2 \\
\end{matrix}\right),\ 
Q^{u_3} = \left(\begin{matrix}
0.7 & 0.1 & 0.2 & 0.  \\
0.5 & 0.2 & 0.1 & 0.2 \\
0.1 & 0.7 & 0.1 & 0.1 \\
0.8 & 0.1 & 0.  & 0.1 \\
\end{matrix}\right)\\ 
\]
Нехай також всі рішення спричиняють однакові витрати:
\[R = \left(\begin{matrix}
1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 \\
\end{matrix}\right)\]
Цим умовам відповідає програма:
\begin{lstlisting}
states = 4
controls = 4

costs = np.array([
    [1, 1, 1, 1],
    [1, 1, 1, 1],
    [1, 1, 1, 1],
    [1, 1, 1, 1]
])


kernels = np.array([
    [[0.1, 0.2, 0.7, 0.7], [0.1, 0.1, 0.1, 0.1], \
     [0.4, 0.3, 0.1, 0.2], [0.4, 0.4, 0.1, 0  ]],
    [[0.7, 0.1, 0.2, 0.5], [0.1, 0  , 0.2, 0.2], \
     [0.1, 0  , 0.5, 0.1], [0.1, 0.9, 0.1, 0.2]],
    [[0.3, 0.3, 0.9, 0.1], [0.1, 0.5, 0  , 0.7], \
     [0.2, 0  , 0.1, 0.1], [0.4, 0.2, 0  , 0.1]],
    [[0.1, 0.1, 0  , 0.8], [0.5, 0.1, 0.6, 0.1], \
     [0.2, 0.7, 0.2, 0  ], [0.2, 0.1, 0.2, 0.1]]
])

strategy = find_optimal_strategy(states, controls, costs, kernels)
print(sorted(strategy))
\end{lstlisting}

Варто зазначити, що вхідна змінна \texttt{kernels} — тривимірний масив, де третій вимір відповідає можливим діям \(u\).

В результаті виконання програми буде виведено на екран:
\begin{lstlisting}
[(0, 3), (1, 1), (2, 2), (3, 2)]
\end{lstlisting}
Тобто \(D_{x_0}^{\delta^{*}}(u_3) = D_{x_1}^{\delta^*}(u_1) = D_{x_2}^{\delta^*}(u_2) = D_{x_3}^{\delta^*}(u_2) = 1 \).

Отже, згідно зі знайденою оптимальною стратегією, в стані системи \(x_0 = (a, a)\) слід обирати дії \(u_3 = (2, 2)\), в стані \(x_1 = (a, b)\) дії \(u_1 = (1, 2)\), в станах \(x_2 = (b, a)\) та \(x_3 = (b, b)\) дії \(u_2 = (2, 1)\). Дії \(u_0 = (1, 1)\) не слід обирати взагалі. \end{example}

\clearpage
\section{Висновки}
У даній роботі було змодельовано систему економічних агентів за допомогою марківського випадкового поля. Було показано, що за певних досить реалістичних припущень щодо такої системи, існує ефективний алгоритм знаходження оптимальної стратегії, яка мінімізує витрати (максимізує доходи), і цей алгоритм працює для довільної кількості станів агентів та довільної кількості можливих дій. Програму знаходження такої оптимальної стратегії в загальному випадку було реалізовано за допомогою бібліотеки \texttt{PuLP} для мови Python, і розглянуто конкретний приклад її знаходження.

Модель може застосовуватися не тільки до вказаної задачі з підприємствами, що взаємодіють, але й до інших ситуацій (агенти можуть бути будь-якими \'акторами деякої економічної системи).

\clearpage


% Sources
\nocite{David:1998}
\nocite{Knopov:2011}
\nocite{Knopov:1998}
\nocite{Chornei:2005}
\nocite{Koller:2009}

\renewcommand\bibname{Посилання}
\bibliography{literature.bib}
\bibliographystyle{ugost2008s}

\end{document}
